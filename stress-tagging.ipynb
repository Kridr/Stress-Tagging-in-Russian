{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-29T21:24:23.325864Z","iopub.execute_input":"2022-12-29T21:24:23.326164Z","iopub.status.idle":"2022-12-29T21:24:23.349676Z","shell.execute_reply.started":"2022-12-29T21:24:23.326096Z","shell.execute_reply":"2022-12-29T21:24:23.348551Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/silero-stress-predictor/sample_submission.csv\n/kaggle/input/silero-stress-predictor/train.csv\n/kaggle/input/silero-stress-predictor/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Research part","metadata":{}},{"cell_type":"markdown","source":"Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\n# import multiprocessing\n# from gensim.models import Word2Vec\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:23.351416Z","iopub.execute_input":"2022-12-29T21:24:23.352304Z","iopub.status.idle":"2022-12-29T21:24:25.960312Z","shell.execute_reply.started":"2022-12-29T21:24:23.352266Z","shell.execute_reply":"2022-12-29T21:24:25.959160Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Reading the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/silero-stress-predictor/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:25.962699Z","iopub.execute_input":"2022-12-29T21:24:25.963588Z","iopub.status.idle":"2022-12-29T21:24:26.200536Z","shell.execute_reply.started":"2022-12-29T21:24:25.963547Z","shell.execute_reply":"2022-12-29T21:24:26.199132Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id      word  stress  num_syllables     lemma\n0   0   румяной       2              3   румяный\n1   1   цифрами       1              3     цифра\n2   2   слугами       1              3     слуга\n3   3  выбирает       3              4  выбирать\n4   4  управдом       3              3  управдом","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>word</th>\n      <th>stress</th>\n      <th>num_syllables</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>румяной</td>\n      <td>2</td>\n      <td>3</td>\n      <td>румяный</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>цифрами</td>\n      <td>1</td>\n      <td>3</td>\n      <td>цифра</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>слугами</td>\n      <td>1</td>\n      <td>3</td>\n      <td>слуга</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>выбирает</td>\n      <td>3</td>\n      <td>4</td>\n      <td>выбирать</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>управдом</td>\n      <td>3</td>\n      <td>3</td>\n      <td>управдом</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"A function for feature extracting","metadata":{}},{"cell_type":"markdown","source":"Setting constants","metadata":{}},{"cell_type":"code","source":"MAX_SYLLABUSES_COUNT = 6\nN_CLASSES = 6","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.204678Z","iopub.execute_input":"2022-12-29T21:24:26.205084Z","iopub.status.idle":"2022-12-29T21:24:26.209949Z","shell.execute_reply.started":"2022-12-29T21:24:26.205044Z","shell.execute_reply":"2022-12-29T21:24:26.208904Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Function for naive syllabus extraction. The main thing is that each syllabus is based on single vowel. Also, it adds padding syllabuses based on constants above","metadata":{}},{"cell_type":"code","source":"def simple_syllabus_extractor(x):\n    vowels = [\"ё\", \"у\", \"е\", \"ы\", \"о\", \"э\", \"я\", \"и\", \"ю\", \"а\"]\n    \n    syllabuses = [''] * MAX_SYLLABUSES_COUNT\n    \n    k = 0\n    sub_syl = ''\n    for e in x:\n        sub_syl += e\n        if e in vowels:\n            syllabuses[k] = sub_syl\n            sub_syl = ''\n            k += 1\n    \n    if sub_syl:\n        syllabuses[k - 1] += sub_syl\n            \n    return syllabuses","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.212816Z","iopub.execute_input":"2022-12-29T21:24:26.213661Z","iopub.status.idle":"2022-12-29T21:24:26.223084Z","shell.execute_reply.started":"2022-12-29T21:24:26.213584Z","shell.execute_reply":"2022-12-29T21:24:26.221947Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"An example of syllabus extraction","metadata":{}},{"cell_type":"code","source":"simple_syllabus_extractor('румяной')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.224521Z","iopub.execute_input":"2022-12-29T21:24:26.225130Z","iopub.status.idle":"2022-12-29T21:24:26.235706Z","shell.execute_reply.started":"2022-12-29T21:24:26.225084Z","shell.execute_reply":"2022-12-29T21:24:26.233730Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['ру', 'мя', 'ной', '', '', '']"},"metadata":{}}]},{"cell_type":"markdown","source":"Setting dictionary that maps token to its index","metadata":{}},{"cell_type":"code","source":"d_ind = 2\nd = {'<UNKN>': 0, '': 1}","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.237488Z","iopub.execute_input":"2022-12-29T21:24:26.237820Z","iopub.status.idle":"2022-12-29T21:24:26.245763Z","shell.execute_reply.started":"2022-12-29T21:24:26.237784Z","shell.execute_reply":"2022-12-29T21:24:26.244779Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Function for numericalizing a word: it just split the word into naive syllabuses and the use the dictionary above to convert the syllabuses to corresponding indexes","metadata":{}},{"cell_type":"code","source":"def numericalize_word(x, fill_dict=False):\n    \"\"\"\n    fill_dict: If True then global dict will be updated using x, otherwise \n    \"\"\"\n    global d_ind\n    \n    syllabuses = simple_syllabus_extractor(x)\n    \n    syllabuses_repr = np.zeros(shape=6, dtype=int)\n        \n    for i, syllabus in enumerate(syllabuses):\n        if syllabus in d:\n            syllabuses_repr[i] = d[syllabus]\n        else:\n            if fill_dict:\n                d[syllabus] = d_ind\n                syllabuses_repr[i] = d_ind\n                d_ind += 1\n            else:\n                syllabuses_repr[i] = d['<UNKN>']\n    \n    return syllabuses_repr","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.247213Z","iopub.execute_input":"2022-12-29T21:24:26.248525Z","iopub.status.idle":"2022-12-29T21:24:26.257184Z","shell.execute_reply.started":"2022-12-29T21:24:26.248486Z","shell.execute_reply":"2022-12-29T21:24:26.256437Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Full feature set","metadata":{}},{"cell_type":"code","source":"X = df['word']\nX[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.258740Z","iopub.execute_input":"2022-12-29T21:24:26.259427Z","iopub.status.idle":"2022-12-29T21:24:26.275068Z","shell.execute_reply.started":"2022-12-29T21:24:26.259391Z","shell.execute_reply":"2022-12-29T21:24:26.273836Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0     румяной\n1     цифрами\n2     слугами\n3    выбирает\n4    управдом\nName: word, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"Full target set","metadata":{}},{"cell_type":"code","source":"y = (df['stress'] - 1).to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.277018Z","iopub.execute_input":"2022-12-29T21:24:26.277302Z","iopub.status.idle":"2022-12-29T21:24:26.287266Z","shell.execute_reply.started":"2022-12-29T21:24:26.277263Z","shell.execute_reply":"2022-12-29T21:24:26.286218Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Function for splitting data into train, test and validation part","metadata":{}},{"cell_type":"code","source":"def train_val_test_split(x, y, train_size=0.7, val_size=0.1):\n    #test = 1 - train - val\n    x_train, x_, y_train, y_ = train_test_split(x, y, train_size=train_size, stratify=y, shuffle=True)\n    x_val, x_test, y_val, y_test = train_test_split(x_, y_, train_size=val_size/(1-train_size), stratify=y_, shuffle=True)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.292918Z","iopub.execute_input":"2022-12-29T21:24:26.293423Z","iopub.status.idle":"2022-12-29T21:24:26.300651Z","shell.execute_reply.started":"2022-12-29T21:24:26.293388Z","shell.execute_reply":"2022-12-29T21:24:26.299503Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Splitting full data","metadata":{}},{"cell_type":"code","source":"X_train_, y_train, X_val_, y_val, X_test_, y_test = train_val_test_split(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.302072Z","iopub.execute_input":"2022-12-29T21:24:26.302442Z","iopub.status.idle":"2022-12-29T21:24:26.348826Z","shell.execute_reply.started":"2022-12-29T21:24:26.302409Z","shell.execute_reply":"2022-12-29T21:24:26.347815Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Function for numericalizing feature sets","metadata":{}},{"cell_type":"code","source":"def apply_boosted(X, fill_dict):\n    res = []\n    for i in X.index:\n        res.append(numericalize_word(X.at[i], fill_dict))\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.349993Z","iopub.execute_input":"2022-12-29T21:24:26.350317Z","iopub.status.idle":"2022-12-29T21:24:26.360851Z","shell.execute_reply.started":"2022-12-29T21:24:26.350281Z","shell.execute_reply":"2022-12-29T21:24:26.359955Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Numericalizing feature sets. Remember that we need to create dictionary only based on training data","metadata":{}},{"cell_type":"code","source":"X_train = apply_boosted(X_train_, True)\nX_val = apply_boosted(X_val_, False)\nX_test = apply_boosted(X_test_, False)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:26.365524Z","iopub.execute_input":"2022-12-29T21:24:26.367740Z","iopub.status.idle":"2022-12-29T21:24:27.475734Z","shell.execute_reply.started":"2022-12-29T21:24:26.367706Z","shell.execute_reply":"2022-12-29T21:24:27.474652Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train[:5], X_test[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:27.477173Z","iopub.execute_input":"2022-12-29T21:24:27.477516Z","iopub.status.idle":"2022-12-29T21:24:27.491344Z","shell.execute_reply.started":"2022-12-29T21:24:27.477483Z","shell.execute_reply":"2022-12-29T21:24:27.489785Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"([array([2, 3, 4, 5, 6, 1]),\n  array([2, 7, 8, 1, 1, 1]),\n  array([ 9, 10,  1,  1,  1,  1]),\n  array([11, 12, 13,  1,  1,  1]),\n  array([ 2, 14, 15,  1,  1,  1])],\n [array([1028,  255,    5,    1,    1,    1]),\n  array([  9, 362,  26,  90, 408,  43]),\n  array([ 111, 1923,   93,    6,    1,    1]),\n  array([  11, 2667, 1023,    1,    1,    1]),\n  array([ 57,  50, 226, 579,  94,   1])])"},"metadata":{}}]},{"cell_type":"markdown","source":"Defining the size of the dictionary to use it in the model","metadata":{}},{"cell_type":"code","source":"LEN_D = len(d) + 1","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:27.495085Z","iopub.execute_input":"2022-12-29T21:24:27.495423Z","iopub.status.idle":"2022-12-29T21:24:27.502280Z","shell.execute_reply.started":"2022-12-29T21:24:27.495390Z","shell.execute_reply":"2022-12-29T21:24:27.500878Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Setting device","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:27.504048Z","iopub.execute_input":"2022-12-29T21:24:27.505212Z","iopub.status.idle":"2022-12-29T21:24:27.573766Z","shell.execute_reply.started":"2022-12-29T21:24:27.505178Z","shell.execute_reply":"2022-12-29T21:24:27.572523Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Creating a class for dataset","metadata":{}},{"cell_type":"code","source":"class StressDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n        self.x = torch.tensor(self.x, dtype=torch.long).to(device)\n        self.y = torch.tensor(self.y, dtype=torch.long).to(device)\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        return (self.x[idx], self.y[idx])","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:27.578173Z","iopub.execute_input":"2022-12-29T21:24:27.578810Z","iopub.status.idle":"2022-12-29T21:24:27.590658Z","shell.execute_reply.started":"2022-12-29T21:24:27.578773Z","shell.execute_reply":"2022-12-29T21:24:27.589693Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Creating datasets from the above data","metadata":{}},{"cell_type":"code","source":"dataset_train = StressDataset(X_train, y_train)\ndataset_val = StressDataset(X_val, y_val)\ndataset_test = StressDataset(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:27.595451Z","iopub.execute_input":"2022-12-29T21:24:27.595792Z","iopub.status.idle":"2022-12-29T21:24:30.498324Z","shell.execute_reply.started":"2022-12-29T21:24:27.595737Z","shell.execute_reply":"2022-12-29T21:24:30.497132Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Creating dataloader. Batch size is pretty big, but the model is relatively simple, so we can afford it","metadata":{}},{"cell_type":"code","source":"batch_size = 1024\n\ndataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\ndataloader_val = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:30.503736Z","iopub.execute_input":"2022-12-29T21:24:30.504141Z","iopub.status.idle":"2022-12-29T21:24:30.514090Z","shell.execute_reply.started":"2022-12-29T21:24:30.504105Z","shell.execute_reply":"2022-12-29T21:24:30.513119Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Creating classification model.\n\nIt just uses Embedding layer with several Linear layers at the end","metadata":{}},{"cell_type":"code","source":"class StressTaggingModel(nn.Module):\n    def __init__(self, embedding_dim, hidden_layer_size):\n        super().__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.hidden_layer_size = hidden_layer_size\n        \n        self.emb = nn.Embedding(num_embeddings=LEN_D, embedding_dim=self.embedding_dim)\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.3)\n        self.fc1 = nn.Linear(self.embedding_dim * MAX_SYLLABUSES_COUNT, self.hidden_layer_size)\n        self.act1 = nn.LeakyReLU()\n        self.fc2 = nn.Linear(self.hidden_layer_size, N_CLASSES)\n        #self.act = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.emb(x)\n        x = self.flatten(x)\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.act1(x)\n        x = self.fc2(x)\n        #x = self.act(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:30.518821Z","iopub.execute_input":"2022-12-29T21:24:30.521261Z","iopub.status.idle":"2022-12-29T21:24:30.534078Z","shell.execute_reply.started":"2022-12-29T21:24:30.521228Z","shell.execute_reply":"2022-12-29T21:24:30.533083Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Setting embedding size, hidden layer size and creating the model with these parameters","metadata":{}},{"cell_type":"code","source":"embedding_dim = 1024\nhidden_layer_size = MAX_SYLLABUSES_COUNT ** 6\n\nmodel = StressTaggingModel(embedding_dim, hidden_layer_size)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:30.540983Z","iopub.execute_input":"2022-12-29T21:24:30.543306Z","iopub.status.idle":"2022-12-29T21:24:33.815900Z","shell.execute_reply.started":"2022-12-29T21:24:30.543262Z","shell.execute_reply":"2022-12-29T21:24:33.814909Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Setting optimizer and loss function","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:33.817264Z","iopub.execute_input":"2022-12-29T21:24:33.817929Z","iopub.status.idle":"2022-12-29T21:24:33.823670Z","shell.execute_reply.started":"2022-12-29T21:24:33.817891Z","shell.execute_reply":"2022-12-29T21:24:33.822416Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Putting model and loss to device","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\ncriterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:33.825368Z","iopub.execute_input":"2022-12-29T21:24:33.825738Z","iopub.status.idle":"2022-12-29T21:24:34.149358Z","shell.execute_reply.started":"2022-12-29T21:24:33.825677Z","shell.execute_reply":"2022-12-29T21:24:34.148377Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Defining train function. It also collect data to use it in classification report ","metadata":{}},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n\n    model.train()\n\n    all_preds = []\n    all_tags = []\n    \n    for batch in iterator:\n        \n        text = batch[0]\n        tags = batch[1]\n        \n        optimizer.zero_grad()\n        \n        predictions = model(text)\n        \n        all_preds.append(predictions.detach().cpu().numpy())\n        all_tags.append(tags.detach().cpu().numpy())\n\n        loss = criterion(predictions, tags)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n\n        \n    return epoch_loss / len(iterator), np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:34.151058Z","iopub.execute_input":"2022-12-29T21:24:34.151445Z","iopub.status.idle":"2022-12-29T21:24:34.160642Z","shell.execute_reply.started":"2022-12-29T21:24:34.151409Z","shell.execute_reply":"2022-12-29T21:24:34.159807Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Defining function for evaluating the model","metadata":{}},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    \n    model.eval()\n\n    all_preds = []\n    all_tags = []\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            text = batch[0]\n            tags = batch[1]\n            \n            predictions = model(text)\n\n            all_preds.append(predictions.detach().cpu().numpy())\n            all_tags.append(tags.detach().cpu().numpy())\n            \n            loss = criterion(predictions, tags)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator), np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:34.162063Z","iopub.execute_input":"2022-12-29T21:24:34.162633Z","iopub.status.idle":"2022-12-29T21:24:34.172768Z","shell.execute_reply.started":"2022-12-29T21:24:34.162594Z","shell.execute_reply":"2022-12-29T21:24:34.171795Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Performing training of the model. It uses classification_report at each iteration. The best model state is chosen based on validation set","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 35\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    train_loss, train_preds, train_tags = train(model, dataloader_train, optimizer, criterion)\n    valid_loss, _, __ = evaluate(model, dataloader_val, criterion)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut2-model.pt')\n    \n    if epoch % 5 == 0:\n        print(f'Epoch: {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_loss:.3f}')\n\n        print(classification_report(train_tags, train_preds))\n    \n        print(f'\\t Val Loss: {valid_loss:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:24:34.174240Z","iopub.execute_input":"2022-12-29T21:24:34.174687Z","iopub.status.idle":"2022-12-29T21:34:57.950677Z","shell.execute_reply.started":"2022-12-29T21:24:34.174646Z","shell.execute_reply":"2022-12-29T21:34:57.949669Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch: 01\n\tTrain Loss: 7.546\n              precision    recall  f1-score   support\n\n           0       0.54      0.49      0.51     10677\n           1       0.53      0.53      0.53     17481\n           2       0.54      0.49      0.51     12931\n           3       0.45      0.43      0.44      2926\n           4       0.30      0.25      0.27       374\n           5       0.00      0.06      0.00        17\n\n    accuracy                           0.50     44406\n   macro avg       0.39      0.38      0.38     44406\nweighted avg       0.53      0.50      0.51     44406\n\n\t Val Loss: 0.746\nEpoch: 06\n\tTrain Loss: 0.349\n              precision    recall  f1-score   support\n\n           0       0.80      0.80      0.80     10677\n           1       0.83      0.85      0.84     17481\n           2       0.90      0.88      0.89     12931\n           3       0.95      0.93      0.94      2926\n           4       0.94      0.92      0.93       374\n           5       0.78      0.82      0.80        17\n\n    accuracy                           0.85     44406\n   macro avg       0.87      0.87      0.87     44406\nweighted avg       0.85      0.85      0.85     44406\n\n\t Val Loss: 0.262\nEpoch: 11\n\tTrain Loss: 0.191\n              precision    recall  f1-score   support\n\n           0       0.90      0.90      0.90     10677\n           1       0.92      0.92      0.92     17481\n           2       0.95      0.95      0.95     12931\n           3       0.97      0.97      0.97      2926\n           4       0.93      0.93      0.93       374\n           5       0.82      0.82      0.82        17\n\n    accuracy                           0.93     44406\n   macro avg       0.91      0.91      0.91     44406\nweighted avg       0.93      0.93      0.93     44406\n\n\t Val Loss: 0.147\nEpoch: 16\n\tTrain Loss: 0.125\n              precision    recall  f1-score   support\n\n           0       0.92      0.92      0.92     10677\n           1       0.94      0.94      0.94     17481\n           2       0.98      0.98      0.98     12931\n           3       0.99      0.99      0.99      2926\n           4       0.97      0.97      0.97       374\n           5       0.79      0.88      0.83        17\n\n    accuracy                           0.95     44406\n   macro avg       0.93      0.95      0.94     44406\nweighted avg       0.95      0.95      0.95     44406\n\n\t Val Loss: 0.074\nEpoch: 21\n\tTrain Loss: 0.054\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97     10677\n           1       0.98      0.98      0.98     17481\n           2       0.99      0.99      0.99     12931\n           3       0.99      0.99      0.99      2926\n           4       0.97      0.97      0.97       374\n           5       0.94      0.88      0.91        17\n\n    accuracy                           0.98     44406\n   macro avg       0.97      0.97      0.97     44406\nweighted avg       0.98      0.98      0.98     44406\n\n\t Val Loss: 0.033\nEpoch: 26\n\tTrain Loss: 0.056\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96     10677\n           1       0.97      0.97      0.97     17481\n           2       1.00      0.99      0.99     12931\n           3       1.00      1.00      1.00      2926\n           4       0.99      0.99      0.99       374\n           5       1.00      1.00      1.00        17\n\n    accuracy                           0.98     44406\n   macro avg       0.99      0.99      0.99     44406\nweighted avg       0.98      0.98      0.98     44406\n\n\t Val Loss: 0.021\nEpoch: 31\n\tTrain Loss: 0.145\n              precision    recall  f1-score   support\n\n           0       0.92      0.92      0.92     10677\n           1       0.94      0.95      0.94     17481\n           2       0.98      0.98      0.98     12931\n           3       0.99      0.99      0.99      2926\n           4       0.96      0.97      0.97       374\n           5       0.88      0.88      0.88        17\n\n    accuracy                           0.95     44406\n   macro avg       0.95      0.95      0.95     44406\nweighted avg       0.95      0.95      0.95     44406\n\n\t Val Loss: 0.071\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluating the model on the train set","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('tut2-model.pt'))\n\ntest_loss, test_preds, test_tags = evaluate(model, dataloader_test, criterion)\n\nprint(f'Test Loss: {test_loss:.3f}')\n\nprint(test_preds[10:])\nprint(test_tags[10:])\n\nprint(classification_report(test_tags, test_preds))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:34:57.952283Z","iopub.execute_input":"2022-12-29T21:34:57.952652Z","iopub.status.idle":"2022-12-29T21:34:59.881932Z","shell.execute_reply.started":"2022-12-29T21:34:57.952607Z","shell.execute_reply":"2022-12-29T21:34:59.880576Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Test Loss: 1.017\n[3 0 1 ... 1 2 0]\n[3 1 2 ... 1 2 0]\n              precision    recall  f1-score   support\n\n           0       0.77      0.80      0.79      3051\n           1       0.80      0.84      0.82      4995\n           2       0.89      0.83      0.86      3695\n           3       0.88      0.84      0.86       836\n           4       0.74      0.68      0.71       107\n           5       0.00      0.00      0.00         4\n\n    accuracy                           0.82     12688\n   macro avg       0.68      0.66      0.67     12688\nweighted avg       0.83      0.82      0.83     12688\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Full training","metadata":{}},{"cell_type":"markdown","source":"In this part we just use the labeled data to train the model and predicting targets for unlabeled data.\n\nThe pipeline is almost exact as the one used above","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/silero-stress-predictor/test.csv')\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:34:59.887464Z","iopub.execute_input":"2022-12-29T21:34:59.888407Z","iopub.status.idle":"2022-12-29T21:34:59.954383Z","shell.execute_reply.started":"2022-12-29T21:34:59.888368Z","shell.execute_reply":"2022-12-29T21:34:59.953504Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"   id           word  num_syllables        lemma\n0   0      эпилепсия              5    эпилепсия\n1   1    относящейся              5   относиться\n2   2  размышлениями              6  размышление\n3   3         модемы              3        модем\n4   4          солнц              1       солнце","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>word</th>\n      <th>num_syllables</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>эпилепсия</td>\n      <td>5</td>\n      <td>эпилепсия</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>относящейся</td>\n      <td>5</td>\n      <td>относиться</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>размышлениями</td>\n      <td>6</td>\n      <td>размышление</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>модемы</td>\n      <td>3</td>\n      <td>модем</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>солнц</td>\n      <td>1</td>\n      <td>солнце</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"d_ind = 2\nd = {'<UNKN>': 0, '': 1}","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:34:59.955735Z","iopub.execute_input":"2022-12-29T21:34:59.956083Z","iopub.status.idle":"2022-12-29T21:34:59.961351Z","shell.execute_reply.started":"2022-12-29T21:34:59.956049Z","shell.execute_reply":"2022-12-29T21:34:59.960142Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"X_train_full = apply_boosted(X, True)\nX_test_full = apply_boosted(df_test['word'], False)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:34:59.962886Z","iopub.execute_input":"2022-12-29T21:34:59.963558Z","iopub.status.idle":"2022-12-29T21:35:00.910974Z","shell.execute_reply.started":"2022-12-29T21:34:59.963501Z","shell.execute_reply":"2022-12-29T21:35:00.909988Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"X_train_full[:5], X_test_full[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:00.912481Z","iopub.execute_input":"2022-12-29T21:35:00.912875Z","iopub.status.idle":"2022-12-29T21:35:00.920896Z","shell.execute_reply.started":"2022-12-29T21:35:00.912839Z","shell.execute_reply":"2022-12-29T21:35:00.919621Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"([array([2, 3, 4, 1, 1, 1]),\n  array([5, 6, 7, 1, 1, 1]),\n  array([8, 9, 7, 1, 1, 1]),\n  array([10, 11, 12, 13,  1,  1]),\n  array([14, 15, 16,  1,  1,  1])],\n [array([ 36, 195,  50, 234, 108,   1]),\n  array([ 64, 115,  93,  92, 994,   1]),\n  array([ 12, 604, 118,  51, 108,   7]),\n  array([ 77,  49, 485,   1,   1,   1]),\n  array([0, 1, 1, 1, 1, 1])])"},"metadata":{}}]},{"cell_type":"code","source":"y[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:00.922650Z","iopub.execute_input":"2022-12-29T21:35:00.923654Z","iopub.status.idle":"2022-12-29T21:35:00.931579Z","shell.execute_reply.started":"2022-12-29T21:35:00.923619Z","shell.execute_reply":"2022-12-29T21:35:00.930695Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"array([1, 0, 0, 2, 2])"},"metadata":{}}]},{"cell_type":"code","source":"dataset_train_full = StressDataset(X_train_full, y)\n#Fictive y_test in order to create dataloader and to form id further\ndataset_test_full = StressDataset(X_test_full, df_test['id'].to_numpy(dtype=int))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:00.932870Z","iopub.execute_input":"2022-12-29T21:35:00.933776Z","iopub.status.idle":"2022-12-29T21:35:01.054956Z","shell.execute_reply.started":"2022-12-29T21:35:00.933723Z","shell.execute_reply":"2022-12-29T21:35:01.054156Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"dataloader_train_full = DataLoader(dataset_train_full, batch_size=batch_size, shuffle=False)\ndataloader_test_full = DataLoader(dataset_test_full, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:01.056288Z","iopub.execute_input":"2022-12-29T21:35:01.056924Z","iopub.status.idle":"2022-12-29T21:35:01.063522Z","shell.execute_reply.started":"2022-12-29T21:35:01.056891Z","shell.execute_reply":"2022-12-29T21:35:01.062664Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"LEN_D = len(d) + 1\nLEN_D","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:01.064576Z","iopub.execute_input":"2022-12-29T21:35:01.064884Z","iopub.status.idle":"2022-12-29T21:35:01.074345Z","shell.execute_reply.started":"2022-12-29T21:35:01.064851Z","shell.execute_reply":"2022-12-29T21:35:01.073150Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"7997"},"metadata":{}}]},{"cell_type":"code","source":"model_full = StressTaggingModel(embedding_dim, hidden_layer_size)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:01.075889Z","iopub.execute_input":"2022-12-29T21:35:01.076298Z","iopub.status.idle":"2022-12-29T21:35:04.230146Z","shell.execute_reply.started":"2022-12-29T21:35:01.076245Z","shell.execute_reply":"2022-12-29T21:35:04.229184Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"optimizer_full = optim.Adam(model_full.parameters(), lr=1e-3)\ncriterion_full = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:04.231646Z","iopub.execute_input":"2022-12-29T21:35:04.232030Z","iopub.status.idle":"2022-12-29T21:35:04.239371Z","shell.execute_reply.started":"2022-12-29T21:35:04.231994Z","shell.execute_reply":"2022-12-29T21:35:04.238481Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model_full = model_full.to(device)\ncriterion_full = criterion_full.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:04.242576Z","iopub.execute_input":"2022-12-29T21:35:04.242903Z","iopub.status.idle":"2022-12-29T21:35:04.586207Z","shell.execute_reply.started":"2022-12-29T21:35:04.242871Z","shell.execute_reply":"2022-12-29T21:35:04.585096Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 35\n\nbest_full_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    train_full_loss, train_full_preds, train_full_tags = train(model_full, dataloader_train_full, optimizer_full, criterion_full)\n        \n    if train_full_loss < best_full_loss:\n        best_full_loss = train_full_loss\n        torch.save(model_full.state_dict(), 'tut3-model.pt')\n    \n    if epoch % 5 == 0:\n        print(f'Epoch: {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_full_loss:.3f}')\n\n        print(classification_report(train_full_tags, train_full_preds))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:35:04.587543Z","iopub.execute_input":"2022-12-29T21:35:04.587930Z","iopub.status.idle":"2022-12-29T21:46:45.693381Z","shell.execute_reply.started":"2022-12-29T21:35:04.587891Z","shell.execute_reply":"2022-12-29T21:46:45.691829Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch: 01\n\tTrain Loss: 4.835\n              precision    recall  f1-score   support\n\n           0       0.55      0.50      0.52     15253\n           1       0.55      0.59      0.57     24973\n           2       0.60      0.57      0.59     18473\n           3       0.63      0.51      0.56      4180\n           4       0.29      0.34      0.31       535\n           5       0.00      0.00      0.00        24\n\n    accuracy                           0.56     63438\n   macro avg       0.44      0.42      0.43     63438\nweighted avg       0.56      0.56      0.56     63438\n\nEpoch: 06\n\tTrain Loss: 0.300\n              precision    recall  f1-score   support\n\n           0       0.83      0.83      0.83     15253\n           1       0.86      0.87      0.86     24973\n           2       0.93      0.91      0.92     18473\n           3       0.95      0.94      0.95      4180\n           4       0.92      0.90      0.91       535\n           5       0.62      0.67      0.64        24\n\n    accuracy                           0.88     63438\n   macro avg       0.85      0.85      0.85     63438\nweighted avg       0.88      0.88      0.88     63438\n\nEpoch: 11\n\tTrain Loss: 0.187\n              precision    recall  f1-score   support\n\n           0       0.88      0.90      0.89     15253\n           1       0.92      0.91      0.92     24973\n           2       0.97      0.96      0.96     18473\n           3       0.98      0.98      0.98      4180\n           4       0.95      0.94      0.94       535\n           5       0.91      0.88      0.89        24\n\n    accuracy                           0.93     63438\n   macro avg       0.93      0.93      0.93     63438\nweighted avg       0.93      0.93      0.93     63438\n\nEpoch: 16\n\tTrain Loss: 0.100\n              precision    recall  f1-score   support\n\n           0       0.94      0.95      0.95     15253\n           1       0.96      0.96      0.96     24973\n           2       0.99      0.98      0.98     18473\n           3       0.98      0.98      0.98      4180\n           4       0.95      0.95      0.95       535\n           5       0.92      0.92      0.92        24\n\n    accuracy                           0.97     63438\n   macro avg       0.96      0.96      0.96     63438\nweighted avg       0.97      0.97      0.97     63438\n\nEpoch: 21\n\tTrain Loss: 0.111\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.93     15253\n           1       0.95      0.95      0.95     24973\n           2       0.98      0.98      0.98     18473\n           3       0.99      0.99      0.99      4180\n           4       0.99      0.99      0.99       535\n           5       0.91      0.88      0.89        24\n\n    accuracy                           0.96     63438\n   macro avg       0.96      0.95      0.96     63438\nweighted avg       0.96      0.96      0.96     63438\n\nEpoch: 26\n\tTrain Loss: 0.095\n              precision    recall  f1-score   support\n\n           0       0.94      0.95      0.95     15253\n           1       0.96      0.96      0.96     24973\n           2       0.99      0.99      0.99     18473\n           3       0.99      0.99      0.99      4180\n           4       0.97      0.97      0.97       535\n           5       0.88      0.92      0.90        24\n\n    accuracy                           0.97     63438\n   macro avg       0.96      0.96      0.96     63438\nweighted avg       0.97      0.97      0.97     63438\n\nEpoch: 31\n\tTrain Loss: 0.043\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     15253\n           1       0.99      0.99      0.99     24973\n           2       0.99      0.99      0.99     18473\n           3       0.99      0.99      0.99      4180\n           4       0.97      0.97      0.97       535\n           5       0.88      0.96      0.92        24\n\n    accuracy                           0.99     63438\n   macro avg       0.97      0.98      0.98     63438\nweighted avg       0.99      0.99      0.99     63438\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    \n    model.eval()\n\n    all_preds = []\n    all_tags = []\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            text = batch[0]\n            tags = batch[1]\n            \n            predictions = model(text)\n\n            all_preds.append(predictions.detach().cpu().numpy())\n            all_tags.append(tags.detach().cpu().numpy())\n            \n            #loss = criterion(predictions, tags)\n\n            #epoch_loss += loss.item()\n        \n    return np.nan, np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:46:45.704399Z","iopub.execute_input":"2022-12-29T21:46:45.704842Z","iopub.status.idle":"2022-12-29T21:46:45.711884Z","shell.execute_reply.started":"2022-12-29T21:46:45.704799Z","shell.execute_reply":"2022-12-29T21:46:45.710563Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model_full.load_state_dict(torch.load('tut3-model.pt'))\n\ntest_full_loss, test_full_preds, test_full_ids = evaluate(model_full, dataloader_test_full, criterion_full)\n\ntest_full_preds[:10], test_full_ids[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:46:45.713402Z","iopub.execute_input":"2022-12-29T21:46:45.714041Z","iopub.status.idle":"2022-12-29T21:46:50.221740Z","shell.execute_reply.started":"2022-12-29T21:46:45.714000Z","shell.execute_reply":"2022-12-29T21:46:50.220681Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(array([2, 2, 2, 1, 0, 1, 2, 1, 1, 2]), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"},"metadata":{}}]},{"cell_type":"code","source":"df_submission = pd.DataFrame.from_dict({'id': test_full_ids, 'stress': test_full_preds + 1}, orient='columns')\ndf_submission.set_index('id', inplace=True)\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:46:50.223080Z","iopub.execute_input":"2022-12-29T21:46:50.223557Z","iopub.status.idle":"2022-12-29T21:46:50.236553Z","shell.execute_reply.started":"2022-12-29T21:46:50.223521Z","shell.execute_reply":"2022-12-29T21:46:50.235444Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"    stress\nid        \n0        3\n1        3\n2        3\n3        2\n4        1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stress</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:46:50.238149Z","iopub.execute_input":"2022-12-29T21:46:50.238490Z","iopub.status.idle":"2022-12-29T21:46:50.297587Z","shell.execute_reply.started":"2022-12-29T21:46:50.238452Z","shell.execute_reply":"2022-12-29T21:46:50.296596Z"},"trusted":true},"execution_count":44,"outputs":[]}]}